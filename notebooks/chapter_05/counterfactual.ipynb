{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Evaluation Concepts: Off-Policy Evaluation and Incremental Action Value\n",
    "\n",
    "This notebook demonstrates practical implementations of two key counterfactual evaluation concepts:\n",
    "1. **Off-Policy Evaluation (OPE)** - Section 5.4.2\n",
    "2. **Incremental Action Value** - Section 5.4.3\n",
    "\n",
    "We'll use a movie recommendation system as our example, showing how to evaluate a new recommendation policy using logged interaction data without exposing users to the experimental policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize incremental action value results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Incremental Action Value Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Distribution of incremental engagement values\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(incremental_results['incremental_engagement'], bins=50, alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(x=0, color='red', linestyle='--', label='No improvement')\n",
    "ax1.axvline(x=incremental_results['incremental_engagement'].mean(), \n",
    "           color='green', linestyle='--', label=f'Mean: {incremental_results[\"incremental_engagement\"].mean():.3f}')\n",
    "ax1.set_xlabel('Incremental Engagement Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Incremental Values')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Incremental values by time of day\n",
    "ax2 = axes[0, 1]\n",
    "time_groups = incremental_results.groupby('context_time')['incremental_engagement'].mean()\n",
    "ax2.bar(time_groups.index, time_groups.values, alpha=0.7)\n",
    "ax2.set_xlabel('Time of Day')\n",
    "ax2.set_ylabel('Mean Incremental Engagement')\n",
    "ax2.set_title('Incremental Value by Time of Day')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Incremental values by device\n",
    "ax3 = axes[0, 2]\n",
    "device_groups = incremental_results.groupby('context_device')['incremental_engagement'].mean()\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "ax3.bar(device_groups.index, device_groups.values, color=colors, alpha=0.7)\n",
    "ax3.set_xlabel('Device Type')\n",
    "ax3.set_ylabel('Mean Incremental Engagement')\n",
    "ax3.set_title('Incremental Value by Device')\n",
    "\n",
    "# 4. Correlation between actual and estimated outcomes\n",
    "ax4 = axes[1, 0]\n",
    "ax4.scatter(incremental_results['estimated_rec_click_prob'], \n",
    "           incremental_results['actual_clicked'], alpha=0.5)\n",
    "ax4.plot([0, 1], [0, 1], 'r--', label='Perfect prediction')\n",
    "ax4.set_xlabel('Estimated Click Probability')\n",
    "ax4.set_ylabel('Actual Click (0/1)')\n",
    "ax4.set_title('Estimated vs Actual Click Outcomes')\n",
    "ax4.legend()\n",
    "\n",
    "# 5. Watch time: estimated vs actual\n",
    "ax5 = axes[1, 1]\n",
    "ax5.scatter(incremental_results['estimated_rec_watch_time'], \n",
    "           incremental_results['actual_watch_time'], alpha=0.5)\n",
    "max_time = max(incremental_results['estimated_rec_watch_time'].max(), \n",
    "               incremental_results['actual_watch_time'].max())\n",
    "ax5.plot([0, max_time], [0, max_time], 'r--', label='Perfect prediction')\n",
    "ax5.set_xlabel('Estimated Watch Time (min)')\n",
    "ax5.set_ylabel('Actual Watch Time (min)')\n",
    "ax5.set_title('Estimated vs Actual Watch Time')\n",
    "ax5.legend()\n",
    "\n",
    "# 6. Top opportunities scatter plot\n",
    "ax6 = axes[1, 2]\n",
    "scatter = ax6.scatter(incremental_results['incremental_click'], \n",
    "                     incremental_results['incremental_watch_time'],\n",
    "                     c=incremental_results['incremental_engagement'], \n",
    "                     cmap='RdYlGn', alpha=0.6)\n",
    "ax6.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax6.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax6.set_xlabel('Incremental Click Probability')\n",
    "ax6.set_ylabel('Incremental Watch Time (min)')\n",
    "ax6.set_title('Incremental Click vs Watch Time')\n",
    "plt.colorbar(scatter, ax=ax6, label='Incremental Engagement')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top opportunities for improvement\n",
    "print(\"\\nTOP 10 OPPORTUNITIES FOR IMPROVEMENT:\")\n",
    "print(\"(Sessions where alternative recommendations could have performed significantly better)\\n\")\n",
    "\n",
    "top_opportunities = incremental_results.nlargest(10, 'incremental_engagement')\n",
    "\n",
    "for i, (_, opportunity) in enumerate(top_opportunities.iterrows(), 1):\n",
    "    rec_movie = movies_catalog[movies_catalog['movie_id'] == opportunity['recommended_movie']].iloc[0]\n",
    "    alt_movie = movies_catalog[movies_catalog['movie_id'] == opportunity['best_alternative']].iloc[0]\n",
    "    \n",
    "    print(f\"{i}. Session {opportunity['session_id']}:\")\n",
    "    print(f\"   Recommended: {rec_movie['title']} ({rec_movie['genre']}, Rating: {rec_movie['rating']:.1f})\")\n",
    "    print(f\"   Alternative: {alt_movie['title']} ({alt_movie['genre']}, Rating: {alt_movie['rating']:.1f})\")\n",
    "    print(f\"   Potential engagement gain: {opportunity['incremental_engagement']:.3f}\")\n",
    "    print(f\"   Context: {opportunity['context_time']} on {opportunity['context_device']}\")\n",
    "    print(f\"   Actual outcome: {'Clicked' if opportunity['actual_clicked'] else 'No click'}, \"\n",
    "          f\"{opportunity['actual_watch_time']:.0f} min watched\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparative Analysis: OPE vs Incremental Action Value\n",
    "\n",
    "Let's compare insights from both evaluation methods and understand how they complement each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison\n",
    "print(\"COMPARATIVE ANALYSIS: OPE vs Incremental Action Value\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. OFF-POLICY EVALUATION INSIGHTS:\")\n",
    "print(\"   • Evaluates overall policy performance\")\n",
    "print(\"   • Provides aggregate metrics across all users/sessions\")\n",
    "print(f\"   • New policy shows {ope_results['engagement_score']['relative_improvement']:.1f}% improvement in engagement\")\n",
    "print(f\"   • Based on {ope_results['engagement_score']['effective_sample_size']:.0f} effective samples\")\n",
    "\n",
    "print(\"\\n2. INCREMENTAL ACTION VALUE INSIGHTS:\")\n",
    "print(\"   • Identifies specific improvement opportunities\")\n",
    "print(\"   • Provides granular, session-level analysis\")\n",
    "print(f\"   • {patterns['overall']['percent_positive_incremental']:.1f}% of sessions have positive incremental value\")\n",
    "print(f\"   • Average potential engagement gain: {patterns['overall']['mean_incremental_engagement']:.4f}\")\n",
    "\n",
    "print(\"\\n3. HOW THEY COMPLEMENT EACH OTHER:\")\n",
    "print(\"   • OPE: 'Should we deploy this new policy overall?'\")\n",
    "print(\"   • IAV: 'Where and when should we apply different strategies?'\")\n",
    "print(\"   • OPE provides confidence for policy changes\")\n",
    "print(\"   • IAV identifies specific optimization opportunities\")\n",
    "\n",
    "# Context-specific insights\n",
    "print(\"\\n4. CONTEXT-SPECIFIC INSIGHTS FROM INCREMENTAL ANALYSIS:\")\n",
    "print(\"\\n   By Time of Day:\")\n",
    "for time_period, avg_incremental in patterns['context_time']['incremental_engagement'].items():\n",
    "    print(f\"   • {time_period.title()}: {avg_incremental:.4f} avg incremental value\")\n",
    "\n",
    "print(\"\\n   By Device Type:\")\n",
    "for device, avg_incremental in patterns['context_device']['incremental_engagement'].items():\n",
    "    print(f\"   • {device.title()}: {avg_incremental:.4f} avg incremental value\")\n",
    "\n",
    "print(\"\\n5. ACTIONABLE RECOMMENDATIONS:\")\n",
    "best_time = max(patterns['context_time']['incremental_engagement'].items(), key=lambda x: x[1])\n",
    "best_device = max(patterns['context_device']['incremental_engagement'].items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"   • Focus policy improvements during {best_time[0]} (highest incremental value)\")\n",
    "print(f\"   • Prioritize optimization for {best_device[0]} users\")\n",
    "print(f\"   • Consider A/B testing the new policy (OPE shows {ope_results['engagement_score']['relative_improvement']:.1f}% improvement)\")\n",
    "print(f\"   • Use incremental analysis to personalize recommendations by context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Key Takeaways\n",
    "\n",
    "This notebook demonstrated two powerful counterfactual evaluation techniques:\n",
    "\n",
    "### Off-Policy Evaluation (OPE)\n",
    "- **Purpose**: Evaluate overall performance of a new policy without deploying it\n",
    "- **Method**: Uses inverse propensity weighting to correct for policy differences\n",
    "- **Output**: Aggregate metrics comparing new vs. existing policy\n",
    "- **Use Case**: Deciding whether to deploy a new recommendation algorithm\n",
    "\n",
    "### Incremental Action Value\n",
    "- **Purpose**: Identify specific opportunities for improvement at the action level\n",
    "- **Method**: Estimates counterfactual outcomes for alternative actions\n",
    "- **Output**: Action-level insights and context-specific patterns\n",
    "- **Use Case**: Understanding where and when different strategies would work better\n",
    "\n",
    "### Best Practices Demonstrated\n",
    "1. **Comprehensive Logging**: We logged actions, probabilities, context, and outcomes\n",
    "2. **Bias Correction**: Used importance weighting to handle policy differences\n",
    "3. **Context Awareness**: Analyzed patterns across different user contexts\n",
    "4. **Validation**: Compared estimated vs. actual outcomes to validate models\n",
    "\n",
    "### Engineering Considerations\n",
    "- **Data Volume**: Counterfactual logging generates large amounts of data\n",
    "- **Computational Cost**: Calculating importance weights and counterfactuals is computationally intensive\n",
    "- **Sample Size**: Effective sample sizes may be much smaller than total samples\n",
    "- **Model Assumptions**: Results depend on the quality of counterfactual estimation models\n",
    "\n",
    "These techniques provide powerful ways to evaluate and improve recommendation systems without the cost and risk of live experimentation, making them valuable tools for data-driven product optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
},
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import binom\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Logged Data\n",
    "\n",
    "First, we'll create synthetic logged data that represents a movie recommendation system. This data will include:\n",
    "- User context (preferences, demographics)\n",
    "- Available movie options\n",
    "- Logged policy decisions and probabilities\n",
    "- User interactions (clicks, ratings, watch time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieRecommendationLogger:\n",
    "    \"\"\"Simulates logged data from a movie recommendation system\"\"\"\n",
    "    \n",
    "    def __init__(self, num_users=1000, num_movies=50, num_sessions=5000):\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.num_sessions = num_sessions\n",
    "        \n",
    "        # Generate movie catalog\n",
    "        self.movies = self._generate_movies()\n",
    "        \n",
    "        # Generate user profiles\n",
    "        self.users = self._generate_users()\n",
    "        \n",
    "        # Generate logged interaction data\n",
    "        self.logged_data = self._generate_logged_data()\n",
    "    \n",
    "    def _generate_movies(self):\n",
    "        \"\"\"Generate a catalog of movies with features\"\"\"\n",
    "        genres = ['Action', 'Comedy', 'Drama', 'Horror', 'Romance', 'Sci-Fi', 'Thriller']\n",
    "        \n",
    "        movies = []\n",
    "        for i in range(self.num_movies):\n",
    "            movie = {\n",
    "                'movie_id': i,\n",
    "                'title': f'Movie_{i}',\n",
    "                'genre': np.random.choice(genres),\n",
    "                'rating': np.random.uniform(3.0, 9.0),  # IMDb-style rating\n",
    "                'popularity': np.random.exponential(1),  # Popularity score\n",
    "                'release_year': np.random.randint(1990, 2024)\n",
    "            }\n",
    "            movies.append(movie)\n",
    "        \n",
    "        return pd.DataFrame(movies)\n",
    "    \n",
    "    def _generate_users(self):\n",
    "        \"\"\"Generate user profiles with preferences\"\"\"\n",
    "        age_groups = ['18-25', '26-35', '36-45', '46-55', '55+']\n",
    "        genres = self.movies['genre'].unique()\n",
    "        \n",
    "        users = []\n",
    "        for i in range(self.num_users):\n",
    "            # Users have preferred genres\n",
    "            preferred_genres = np.random.choice(genres, size=np.random.randint(1, 4), replace=False)\n",
    "            \n",
    "            user = {\n",
    "                'user_id': i,\n",
    "                'age_group': np.random.choice(age_groups),\n",
    "                'preferred_genres': list(preferred_genres),\n",
    "                'rating_tendency': np.random.uniform(0.3, 0.9),  # How likely to rate highly\n",
    "                'engagement_level': np.random.uniform(0.2, 1.0)  # How likely to interact\n",
    "            }\n",
    "            users.append(user)\n",
    "        \n",
    "        return pd.DataFrame(users)\n",
    "    \n",
    "    def _calculate_user_movie_affinity(self, user_row, movie_row):\n",
    "        \"\"\"Calculate how much a user would like a movie (hidden ground truth)\"\"\"\n",
    "        base_affinity = 0.3\n",
    "        \n",
    "        # Genre preference boost\n",
    "        if movie_row['genre'] in user_row['preferred_genres']:\n",
    "            base_affinity += 0.4\n",
    "        \n",
    "        # Rating quality boost\n",
    "        rating_boost = (movie_row['rating'] - 5.0) / 10.0  # Normalize around 5.0\n",
    "        base_affinity += rating_boost * 0.3\n",
    "        \n",
    "        # User engagement factor\n",
    "        base_affinity *= user_row['engagement_level']\n",
    "        \n",
    "        return np.clip(base_affinity, 0.0, 1.0)\n",
    "    \n",
    "    def _logged_policy_probabilities(self, user_row, available_movies, context):\n",
    "        \"\"\"Simulate the logged policy's decision probabilities\"\"\"\n",
    "        # Simple popularity-based policy with some randomization\n",
    "        scores = []\n",
    "        for _, movie in available_movies.iterrows():\n",
    "            score = movie['popularity']\n",
    "            \n",
    "            # Add some genre preference\n",
    "            if movie['genre'] in user_row['preferred_genres']:\n",
    "                score *= 1.5\n",
    "            \n",
    "            # Add some randomness\n",
    "            score *= np.random.uniform(0.8, 1.2)\n",
    "            scores.append(score)\n",
    "        \n",
    "        # Convert to probabilities using softmax\n",
    "        scores = np.array(scores)\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Numerical stability\n",
    "        probabilities = exp_scores / np.sum(exp_scores)\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def _generate_logged_data(self):\n",
    "        \"\"\"Generate logged interaction data\"\"\"\n",
    "        logged_sessions = []\n",
    "        \n",
    "        for session_id in range(self.num_sessions):\n",
    "            # Random user and context\n",
    "            user_id = np.random.randint(0, self.num_users)\n",
    "            user_row = self.users.iloc[user_id]\n",
    "            \n",
    "            # Context for this session\n",
    "            context = {\n",
    "                'time_of_day': np.random.choice(['morning', 'afternoon', 'evening', 'night']),\n",
    "                'device': np.random.choice(['mobile', 'desktop', 'tv']),\n",
    "                'day_of_week': np.random.choice(['weekday', 'weekend'])\n",
    "            }\n",
    "            \n",
    "            # Available movies for recommendation (random subset)\n",
    "            num_available = np.random.randint(5, 15)\n",
    "            available_movie_indices = np.random.choice(self.num_movies, size=num_available, replace=False)\n",
    "            available_movies = self.movies.iloc[available_movie_indices]\n",
    "            \n",
    "            # Get logged policy probabilities\n",
    "            action_probs = self._logged_policy_probabilities(user_row, available_movies, context)\n",
    "            \n",
    "            # Sample action from logged policy\n",
    "            chosen_idx = np.random.choice(len(available_movies), p=action_probs)\n",
    "            chosen_movie = available_movies.iloc[chosen_idx]\n",
    "            chosen_prob = action_probs[chosen_idx]\n",
    "            \n",
    "            # Calculate ground truth affinity\n",
    "            affinity = self._calculate_user_movie_affinity(user_row, chosen_movie)\n",
    "            \n",
    "            # Simulate user interactions based on affinity\n",
    "            clicked = np.random.random() < affinity\n",
    "            if clicked:\n",
    "                watch_time = np.random.exponential(affinity * 120)  # Minutes watched\n",
    "                rating = None\n",
    "                if np.random.random() < 0.3:  # 30% chance of rating\n",
    "                    base_rating = affinity * 5 + np.random.normal(0, 0.5)\n",
    "                    rating = np.clip(base_rating, 1, 5)\n",
    "            else:\n",
    "                watch_time = 0\n",
    "                rating = None\n",
    "            \n",
    "            # Store logged data\n",
    "            session_data = {\n",
    "                'session_id': session_id,\n",
    "                'user_id': user_id,\n",
    "                'context_time': context['time_of_day'],\n",
    "                'context_device': context['device'],\n",
    "                'context_day': context['day_of_week'],\n",
    "                'recommended_movie_id': chosen_movie['movie_id'],\n",
    "                'recommended_movie_genre': chosen_movie['genre'],\n",
    "                'recommended_movie_rating': chosen_movie['rating'],\n",
    "                'action_probability': chosen_prob,\n",
    "                'available_movies': list(available_movies['movie_id']),\n",
    "                'all_action_probs': list(action_probs),\n",
    "                'clicked': clicked,\n",
    "                'watch_time': watch_time,\n",
    "                'user_rating': rating,\n",
    "                'true_affinity': affinity  # Hidden ground truth (not available in real logs)\n",
    "            }\n",
    "            \n",
    "            logged_sessions.append(session_data)\n",
    "        \n",
    "        return pd.DataFrame(logged_sessions)\n",
    "\n",
    "# Generate our synthetic dataset\n",
    "logger = MovieRecommendationLogger(num_users=500, num_movies=30, num_sessions=2000)\n",
    "logged_data = logger.logged_data\n",
    "movies_catalog = logger.movies\n",
    "users_catalog = logger.users\n",
    "\n",
    "print(f\"Generated {len(logged_data)} logged sessions\")\n",
    "print(f\"Movie catalog: {len(movies_catalog)} movies\")\n",
    "print(f\"User base: {len(users_catalog)} users\")\n",
    "print(f\"\\nClick-through rate: {logged_data['clicked'].mean():.3f}\")\n",
    "print(f\"Average watch time: {logged_data['watch_time'].mean():.1f} minutes\")\n",
    "\n",
    "# Display sample of logged data\n",
    "print(\"\\nSample logged data:\")\n",
    "display_cols = ['session_id', 'user_id', 'recommended_movie_id', 'action_probability', \n",
    "                'clicked', 'watch_time', 'context_device']\n",
    "logged_data[display_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Off-Policy Evaluation (OPE) - Section 5.4.2\n",
    "\n",
    "Now we'll implement off-policy evaluation to compare how well a new recommendation policy would perform relative to the logged policy. \n",
    "\n",
    "Our new policy will prioritize highly-rated movies from the user's preferred genres, while the logged policy was more popularity-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffPolicyEvaluator:\n",
    "    \"\"\"Implements Off-Policy Evaluation for recommendation policies\"\"\"\n",
    "    \n",
    "    def __init__(self, logged_data, movies_catalog, users_catalog):\n",
    "        self.logged_data = logged_data\n",
    "        self.movies_catalog = movies_catalog\n",
    "        self.users_catalog = users_catalog\n",
    "    \n",
    "    def new_policy_probabilities(self, user_id, available_movie_ids, context=None):\n",
    "        \"\"\"Calculate probabilities for our new quality-focused policy\"\"\"\n",
    "        user_prefs = self.users_catalog.iloc[user_id]['preferred_genres']\n",
    "        available_movies = self.movies_catalog[self.movies_catalog['movie_id'].isin(available_movie_ids)]\n",
    "        \n",
    "        scores = []\n",
    "        for _, movie in available_movies.iterrows():\n",
    "            # Start with movie rating as base score\n",
    "            score = movie['rating']\n",
    "            \n",
    "            # Strong boost for preferred genres\n",
    "            if movie['genre'] in user_prefs:\n",
    "                score *= 2.0\n",
    "            \n",
    "            # Slight boost for newer movies\n",
    "            if movie['release_year'] >= 2020:\n",
    "                score *= 1.2\n",
    "                \n",
    "            scores.append(score)\n",
    "        \n",
    "        # Convert to probabilities using softmax\n",
    "        scores = np.array(scores)\n",
    "        if len(scores) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        exp_scores = np.exp(scores - np.max(scores))\n",
    "        probabilities = exp_scores / np.sum(exp_scores)\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def calculate_importance_weights(self):\n",
    "        \"\"\"Calculate importance weights for inverse propensity weighting\"\"\"\n",
    "        weights = []\n",
    "        \n",
    "        for _, session in self.logged_data.iterrows():\n",
    "            # Get new policy probabilities for this session\n",
    "            new_probs = self.new_policy_probabilities(\n",
    "                session['user_id'], \n",
    "                session['available_movies']\n",
    "            )\n",
    "            \n",
    "            if len(new_probs) == 0:\n",
    "                weights.append(0)\n",
    "                continue\n",
    "            \n",
    "            # Find which action was taken\n",
    "            recommended_id = session['recommended_movie_id']\n",
    "            available_ids = session['available_movies']\n",
    "            \n",
    "            try:\n",
    "                action_idx = available_ids.index(recommended_id)\n",
    "                new_policy_prob = new_probs[action_idx]\n",
    "                logged_policy_prob = session['action_probability']\n",
    "                \n",
    "                # Importance weight = P_new(action) / P_logged(action)\n",
    "                if logged_policy_prob > 0:\n",
    "                    weight = new_policy_prob / logged_policy_prob\n",
    "                else:\n",
    "                    weight = 0\n",
    "            except (ValueError, IndexError):\n",
    "                weight = 0\n",
    "            \n",
    "            weights.append(weight)\n",
    "        \n",
    "        return np.array(weights)\n",
    "    \n",
    "    def evaluate_policy(self, metric='click_rate'):\n",
    "        \"\"\"Evaluate the new policy using inverse propensity weighting\"\"\"\n",
    "        weights = self.calculate_importance_weights()\n",
    "        \n",
    "        # Define reward based on metric\n",
    "        if metric == 'click_rate':\n",
    "            rewards = self.logged_data['clicked'].astype(float)\n",
    "        elif metric == 'watch_time':\n",
    "            rewards = self.logged_data['watch_time']\n",
    "        elif metric == 'engagement_score':\n",
    "            # Composite metric: click + normalized watch time\n",
    "            rewards = (self.logged_data['clicked'].astype(float) + \n",
    "                      self.logged_data['watch_time'] / 100)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {metric}\")\n",
    "        \n",
    "        # Calculate weighted average\n",
    "        if np.sum(weights) > 0:\n",
    "            new_policy_value = np.sum(weights * rewards) / np.sum(weights)\n",
    "        else:\n",
    "            new_policy_value = 0\n",
    "        \n",
    "        # Calculate original policy value (unweighted)\n",
    "        original_policy_value = np.mean(rewards)\n",
    "        \n",
    "        # Calculate confidence interval for the estimate\n",
    "        n_samples = len(weights)\n",
    "        effective_sample_size = (np.sum(weights) ** 2) / np.sum(weights ** 2) if np.sum(weights) > 0 else 0\n",
    "        \n",
    "        results = {\n",
    "            'metric': metric,\n",
    "            'original_policy_value': original_policy_value,\n",
    "            'new_policy_value': new_policy_value,\n",
    "            'improvement': new_policy_value - original_policy_value,\n",
    "            'relative_improvement': ((new_policy_value / original_policy_value - 1) * 100 \n",
    "                                   if original_policy_value > 0 else 0),\n",
    "            'effective_sample_size': effective_sample_size,\n",
    "            'total_samples': n_samples,\n",
    "            'mean_weight': np.mean(weights),\n",
    "            'weight_variance': np.var(weights)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize the evaluator\n",
    "ope_evaluator = OffPolicyEvaluator(logged_data, movies_catalog, users_catalog)\n",
    "\n",
    "# Evaluate the new policy on different metrics\n",
    "metrics_to_evaluate = ['click_rate', 'watch_time', 'engagement_score']\n",
    "ope_results = {}\n",
    "\n",
    "for metric in metrics_to_evaluate:\n",
    "    result = ope_evaluator.evaluate_policy(metric)\n",
    "    ope_results[metric] = result\n",
    "    \n",
    "    print(f\"\\n{metric.upper()} EVALUATION RESULTS:\")\n",
    "    print(f\"Original Policy Value: {result['original_policy_value']:.4f}\")\n",
    "    print(f\"New Policy Value: {result['new_policy_value']:.4f}\")\n",
    "    print(f\"Absolute Improvement: {result['improvement']:.4f}\")\n",
    "    print(f\"Relative Improvement: {result['relative_improvement']:.2f}%\")\n",
    "    print(f\"Effective Sample Size: {result['effective_sample_size']:.0f} / {result['total_samples']}\")\n",
    "    print(f\"Mean Importance Weight: {result['mean_weight']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the OPE results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Off-Policy Evaluation Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Policy value comparison\n",
    "ax1 = axes[0, 0]\n",
    "metrics = list(ope_results.keys())\n",
    "original_values = [ope_results[m]['original_policy_value'] for m in metrics]\n",
    "new_values = [ope_results[m]['new_policy_value'] for m in metrics]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, original_values, width, label='Original Policy', alpha=0.7)\n",
    "ax1.bar(x + width/2, new_values, width, label='New Policy', alpha=0.7)\n",
    "ax1.set_xlabel('Metric')\n",
    "ax1.set_ylabel('Policy Value')\n",
    "ax1.set_title('Policy Value Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Relative improvements\n",
    "ax2 = axes[0, 1]\n",
    "improvements = [ope_results[m]['relative_improvement'] for m in metrics]\n",
    "colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "ax2.bar(metrics, improvements, color=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Metric')\n",
    "ax2.set_ylabel('Relative Improvement (%)')\n",
    "ax2.set_title('Relative Performance Improvement')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax2.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "\n",
    "# 3. Importance weights distribution\n",
    "ax3 = axes[1, 0]\n",
    "weights = ope_evaluator.calculate_importance_weights()\n",
    "ax3.hist(weights, bins=50, alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Importance Weight')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Distribution of Importance Weights')\n",
    "ax3.axvline(x=np.mean(weights), color='red', linestyle='--', label=f'Mean: {np.mean(weights):.3f}')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Effective sample size\n",
    "ax4 = axes[1, 1]\n",
    "eff_sizes = [ope_results[m]['effective_sample_size'] for m in metrics]\n",
    "total_size = ope_results[metrics[0]]['total_samples']\n",
    "ax4.bar(metrics, eff_sizes, alpha=0.7)\n",
    "ax4.axhline(y=total_size, color='red', linestyle='--', \n",
    "           label=f'Total Samples: {total_size}')\n",
    "ax4.set_xlabel('Metric')\n",
    "ax4.set_ylabel('Effective Sample Size')\n",
    "ax4.set_title('Effective Sample Size by Metric')\n",
    "ax4.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Incremental Action Value - Section 5.4.3\n",
    "\n",
    "Now we'll implement incremental action value analysis to understand the potential impact of individual recommendation decisions. This helps identify specific scenarios where alternative recommendations might have performed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalActionValueAnalyzer:\n",
    "    \"\"\"Analyzes incremental action values for individual recommendations\"\"\"\n",
    "    \n",
    "    def __init__(self, logged_data, movies_catalog, users_catalog):\n",
    "        self.logged_data = logged_data\n",
    "        self.movies_catalog = movies_catalog\n",
    "        self.users_catalog = users_catalog\n",
    "    \n",
    "    def estimate_counterfactual_outcome(self, user_id, movie_id, context=None):\n",
    "        \"\"\"Estimate what would have happened if we recommended a different movie\"\"\"\n",
    "        user_prefs = self.users_catalog.iloc[user_id]['preferred_genres']\n",
    "        user_engagement = self.users_catalog.iloc[user_id]['engagement_level']\n",
    "        user_rating_tendency = self.users_catalog.iloc[user_id]['rating_tendency']\n",
    "        \n",
    "        movie = self.movies_catalog[self.movies_catalog['movie_id'] == movie_id].iloc[0]\n",
    "        \n",
    "        # Calculate estimated affinity (similar to ground truth calculation)\n",
    "        base_affinity = 0.3\n",
    "        \n",
    "        # Genre preference boost\n",
    "        if movie['genre'] in user_prefs:\n",
    "            base_affinity += 0.4\n",
    "        \n",
    "        # Rating quality boost\n",
    "        rating_boost = (movie['rating'] - 5.0) / 10.0\n",
    "        base_affinity += rating_boost * 0.3\n",
    "        \n",
    "        # User engagement factor\n",
    "        base_affinity *= user_engagement\n",
    "        \n",
    "        # Context adjustments\n",
    "        if context:\n",
    "            if context.get('time_of_day') == 'evening' and movie['genre'] in ['Horror', 'Thriller']:\n",
    "                base_affinity *= 1.2\n",
    "            if context.get('device') == 'tv' and movie['genre'] in ['Action', 'Sci-Fi']:\n",
    "                base_affinity *= 1.1\n",
    "        \n",
    "        affinity = np.clip(base_affinity, 0.0, 1.0)\n",
    "        \n",
    "        # Estimate outcomes\n",
    "        prob_click = affinity\n",
    "        expected_watch_time = affinity * 120  # Minutes\n",
    "        expected_rating = affinity * 4 + 1 if affinity > 0.3 else None\n",
    "        \n",
    "        return {\n",
    "            'prob_click': prob_click,\n",
    "            'expected_watch_time': expected_watch_time,\n",
    "            'expected_rating': expected_rating,\n",
    "            'estimated_affinity': affinity\n",
    "        }\n",
    "    \n",
    "    def calculate_incremental_values(self, sample_size=500):\n",
    "        \"\"\"Calculate incremental action values for a sample of sessions\"\"\"\

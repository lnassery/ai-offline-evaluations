{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shadow Traffic Implementation for AI Model Evaluation\n",
    "\n",
    "This notebook demonstrates a simple implementation of shadow traffic for evaluating AI model performance metrics.\n",
    "\n",
    "Shadow traffic enables you to test a new model implementation by processing duplicate requests without affecting users (just make sure not to ddos product systems!).\n",
    "\n",
    "Unlike chapters 2-3, chapter 4 focuses on system performance that is still very critical to how a model influences a product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Set plot styling for better visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define a Simple Recommendation Model\n",
    "\n",
    "First, we'll create a class that simulates a movie recommendation model with configurable performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationModel:\n",
    "    \"\"\"Simple movie recommendation model simulator with performance tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, name, avg_latency_ms):\n",
    "        self.name = name\n",
    "        self.avg_latency_ms = avg_latency_ms\n",
    "        # Performance tracking metrics\n",
    "        self.request_count = 0\n",
    "        self.error_count = 0\n",
    "        self.latencies = []\n",
    "    \n",
    "    def get_recommendations(self, user_id, context=None):\n",
    "        \"\"\"Generate recommendations with simulated latency\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.request_count += 1\n",
    "        \n",
    "        try:\n",
    "            # Simulate variable latency with some randomness\n",
    "            latency = max(10, np.random.normal(self.avg_latency_ms, self.avg_latency_ms * 0.2)) / 1000.0\n",
    "            time.sleep(latency)\n",
    "            \n",
    "            # Generate mock recommendations\n",
    "            recommendations = [\n",
    "                {\"movie_id\": i, \"title\": f\"Movie {i}\", \"score\": np.random.random()} \n",
    "                for i in range(1, 6)\n",
    "            ]\n",
    "            \n",
    "            # Record actual latency for this request\n",
    "            actual_latency = (time.time() - start_time) * 1000  # in milliseconds\n",
    "            self.latencies.append(actual_latency)\n",
    "            \n",
    "            return recommendations\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            # In a real system, you would log the error\n",
    "            return []\n",
    "    \n",
    "    def get_performance_metrics(self):\n",
    "        \"\"\"Return key performance metrics about this model's behavior\"\"\"\n",
    "        if not self.latencies:\n",
    "            return {\"name\": self.name, \"request_count\": 0}\n",
    "        \n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"request_count\": self.request_count,\n",
    "            \"error_rate\": self.error_count / self.request_count if self.request_count > 0 else 0,\n",
    "            \"avg_latency_ms\": np.mean(self.latencies),\n",
    "            \"p50_latency_ms\": np.percentile(self.latencies, 50),\n",
    "            \"p95_latency_ms\": np.percentile(self.latencies, 95),\n",
    "            \"p99_latency_ms\": np.percentile(self.latencies, 99),\n",
    "            \"max_latency_ms\": np.max(self.latencies)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the Shadow Traffic Router\n",
    "\n",
    "Next, we'll implement a router that handles sending duplicate requests to both the production and shadow systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShadowTrafficRouter:\n",
    "    \"\"\"Routes traffic to both production and shadow systems and collects metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, production_model, shadow_model):\n",
    "        self.production_model = production_model\n",
    "        self.shadow_model = shadow_model\n",
    "        self.comparison_data = []\n",
    "    \n",
    "    def route_request(self, user_id, context=None):\n",
    "        \"\"\"Process a request through both production and shadow models\"\"\"\n",
    "        prod_start_time = time.time()\n",
    "        \n",
    "        # Get recommendations from production model (this is what users will see)\n",
    "        prod_recommendations = self.production_model.get_recommendations(user_id, context)\n",
    "        prod_latency = (time.time() - prod_start_time) * 1000  # ms\n",
    "        \n",
    "        # Asynchronously get recommendations from shadow model\n",
    "        # In production, this would typically be done via message queue or async processing\n",
    "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            shadow_start_time = time.time()\n",
    "            future = executor.submit(self.shadow_model.get_recommendations, user_id, context)\n",
    "            shadow_recommendations = future.result(timeout=2.0)  # 2-second timeout\n",
    "            shadow_latency = (time.time() - shadow_start_time) * 1000  # ms\n",
    "        \n",
    "        # Log this request for comparison analysis\n",
    "        self.comparison_data.append({\n",
    "            \"timestamp\": time.time(),\n",
    "            \"user_id\": user_id,\n",
    "            \"context\": str(context),  # Convert to string for DataFrame compatibility\n",
    "            \"prod_latency_ms\": prod_latency,\n",
    "            \"shadow_latency_ms\": shadow_latency,\n",
    "            \"latency_diff_ms\": shadow_latency - prod_latency,\n",
    "            \"prod_recommendation_count\": len(prod_recommendations),\n",
    "            \"shadow_recommendation_count\": len(shadow_recommendations)\n",
    "        })\n",
    "        \n",
    "        # Return only the production recommendations to the user\n",
    "        return prod_recommendations\n",
    "    \n",
    "    def get_performance_comparison(self):\n",
    "        \"\"\"Generate a comparison report between production and shadow models\"\"\"\n",
    "        if not self.comparison_data:\n",
    "            return \"No comparison data available\"\n",
    "        \n",
    "        df = pd.DataFrame(self.comparison_data)\n",
    "        \n",
    "        # Get aggregate metrics\n",
    "        metrics = {\n",
    "            \"request_count\": len(df),\n",
    "            \"avg_prod_latency_ms\": df[\"prod_latency_ms\"].mean(),\n",
    "            \"avg_shadow_latency_ms\": df[\"shadow_latency_ms\"].mean(),\n",
    "            \"p95_prod_latency_ms\": df[\"prod_latency_ms\"].quantile(0.95),\n",
    "            \"p95_shadow_latency_ms\": df[\"shadow_latency_ms\"].quantile(0.95),\n",
    "            \"p99_prod_latency_ms\": df[\"prod_latency_ms\"].quantile(0.99),\n",
    "            \"p99_shadow_latency_ms\": df[\"shadow_latency_ms\"].quantile(0.99),\n",
    "            \"max_prod_latency_ms\": df[\"prod_latency_ms\"].max(),\n",
    "            \"max_shadow_latency_ms\": df[\"shadow_latency_ms\"].max(),\n",
    "            \"avg_latency_increase_ms\": df[\"latency_diff_ms\"].mean(),\n",
    "            \"avg_latency_increase_pct\": (df[\"shadow_latency_ms\"].mean() / df[\"prod_latency_ms\"].mean() - 1) * 100\n",
    "        }\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a Shadow Traffic Evaluation\n",
    "\n",
    "Now we'll create instances of both models and simulate user traffic through our shadow traffic router.\n",
    "\n This may take awhile..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our models - production is faster, shadow is slower but theoretically better\n",
    "production_model = RecommendationModel(\"Current Production Model\", avg_latency_ms=150)  # 150ms avg\n",
    "shadow_model = RecommendationModel(\"New Candidate Model\", avg_latency_ms=220)  # 220ms avg\n",
    "\n",
    "# Create router\n",
    "router = ShadowTrafficRouter(production_model, shadow_model)\n",
    "\n",
    "# Simulate user traffic (500 requests)\n",
    "print(\"Simulating user traffic with shadow evaluation...\")\n",
    "for i in range(500):\n",
    "    user_id = f\"user_{i % 100}\"  # 100 unique users making multiple requests\n",
    "    context = {\"page\": \"homepage\", \"device\": np.random.choice([\"mobile\", \"desktop\", \"tv\"])}\n",
    "    \n",
    "    # Route request through both systems\n",
    "    recommendations = router.route_request(user_id, context)\n",
    "    \n",
    "    # Simulate varying traffic patterns with small delays between requests\n",
    "    time.sleep(np.random.exponential(0.02))\n",
    "    \n",
    "    # Show progress\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"Processed {i+1} requests\")\n",
    "\n",
    "print(\"Simulation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze the Performance Results\n",
    "\n",
    "Let's examine the performance metrics collected during our shadow traffic evaluation.\n\n",
    "The output of this analysis is latency (p50,p95,p99, max) etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metrics from both models\n",
    "prod_metrics = production_model.get_performance_metrics()\n",
    "shadow_metrics = shadow_model.get_performance_metrics()\n",
    "\n",
    "# Format and display production model metrics\n",
    "print(\"Production Model Performance Metrics:\")\n",
    "for key, value in prod_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nShadow Model Performance Metrics:\")\n",
    "for key, value in shadow_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Get the comparison report\n",
    "comparison = router.get_performance_comparison()\n",
    "print(\"\\nPerformance Comparison (Shadow vs Production):\")\n",
    "for key, value in comparison.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the Latency Distributions\n",
    "\n",
    "Visualization is key to understanding performance differences between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of latency comparison\n",
    "df = pd.DataFrame(router.comparison_data)\n",
    "\n",
    "# 1. Latency distribution comparison\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df[\"prod_latency_ms\"], alpha=0.5, bins=30, label=\"Production Model\")\n",
    "plt.hist(df[\"shadow_latency_ms\"], alpha=0.5, bins=30, label=\"Shadow Model\")\n",
    "plt.xlabel(\"Latency (ms)\")\n",
    "plt.ylabel(\"Request Count\")\n",
    "plt.title(\"Latency Distribution Comparison\")\n",
    "plt.legend()\n",
    "\n",
    "# 2. Percentile latency comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "percentiles = range(5, 100, 5)\n",
    "prod_percentiles = [np.percentile(df[\"prod_latency_ms\"], p) for p in percentiles]\n",
    "shadow_percentiles = [np.percentile(df[\"shadow_latency_ms\"], p) for p in percentiles]\n",
    "\n",
    "plt.plot(percentiles, prod_percentiles, 'b-o', label=\"Production Model\")\n",
    "plt.plot(percentiles, shadow_percentiles, 'r-o', label=\"Shadow Model\")\n",
    "plt.xlabel(\"Percentile\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "plt.title(\"Latency by Percentile\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make a Performance-Based Decision\n",
    "\n",
    "Based on our engineering performance metrics, we can make a data-driven decision about whether the new model meets our performance requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define acceptable thresholds (these would be based on your requirements)\n",
    "p99_threshold = 1.3  # Allow up to 30% increase in p99 latency\n",
    "avg_threshold = 1.5  # Allow up to 50% increase in average latency\n",
    "\n",
    "# Check if shadow model meets performance requirements\n",
    "p99_ratio = shadow_metrics[\"p99_latency_ms\"] / prod_metrics[\"p99_latency_ms\"]\n",
    "avg_ratio = shadow_metrics[\"avg_latency_ms\"] / prod_metrics[\"avg_latency_ms\"]\n",
    "\n",
    "print(\"Engineering Performance Evaluation Decision:\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "if p99_ratio > p99_threshold:\n",
    "    print(f\"REJECT (Totally BAD!): p99 latency increase of {(p99_ratio-1)*100:.1f}% exceeds threshold of {(p99_threshold-1)*100:.1f}%\")\n",
    "elif avg_ratio > avg_threshold:\n",
    "    print(f\"REJECT (Totally BAD!): Average latency increase of {(avg_ratio-1)*100:.1f}% exceeds threshold of {(avg_threshold-1)*100:.1f}%\")\n",
    "elif shadow_metrics[\"error_rate\"] > 2 * prod_metrics[\"error_rate\"]:\n",
    "    print(f\"REJECT (Totally BAD!): Error rate too high ({shadow_metrics['error_rate']:.4f} vs {prod_metrics['error_rate']:.4f})\")\n",
    "else:\n",
    "    print(f\"APPROVE (Legit!): Performance impact within acceptable thresholds\")\n",
    "    print(f\"   • Average latency: {prod_metrics['avg_latency_ms']:.1f}ms → {shadow_metrics['avg_latency_ms']:.1f}ms (+{(avg_ratio-1)*100:.1f}%)\")\n",
    "    print(f\"   • p99 latency: {prod_metrics['p99_latency_ms']:.1f}ms → {shadow_metrics['p99_latency_ms']:.1f}ms (+{(p99_ratio-1)*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bonus: Create a Performance Dashboard\n",
    "\n",
    "In a real-world scenario, you might want to visualize the performance metrics over time in a dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time-series view of the latency\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Resample to 30-second intervals for clearer visualization\n",
    "prod_series = df['prod_latency_ms'].resample('30S').mean()\n",
    "shadow_series = df['shadow_latency_ms'].resample('30S').mean()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot moving averages\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(prod_series.index, prod_series, 'b-', label='Production Model')\n",
    "plt.plot(shadow_series.index, shadow_series, 'r-', label='Shadow Model')\n",
    "plt.ylabel('Latency (ms)')\n",
    "plt.title('Average Latency Over Time (30s intervals)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot latency difference\n",
    "plt.subplot(2, 1, 2)\n",
    "diff_series = shadow_series - prod_series\n",
    "plt.fill_between(diff_series.index, diff_series, 0, where=diff_series>=0, \n",
    "                  color='r', alpha=0.3, label='Shadow slower')\n",
    "plt.fill_between(diff_series.index, diff_series, 0, where=diff_series<0, \n",
    "                  color='g', alpha=0.3, label='Shadow faster')\n",
    "plt.plot(diff_series.index, diff_series, 'k-')\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.ylabel('Latency Difference (ms)')\n",
    "plt.xlabel('Time')\n",
    "plt.title('Shadow Model vs Production Model Latency Difference')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to implement shadow traffic evaluation for AI models with a focus on engineering performance metrics. Using this approach, you can:\n",
    "\n",
    "1. Test new model implementations in a production-like environment with real user requests\n",
    "2. Collect detailed performance metrics without affecting the user experience\n",
    "3. Make data-driven decisions about whether a model meets your engineering requirements\n",
    "4. Identify performance issues before exposing users to a new model implementation\n",
    "\n",
    "Once a model passes engineering performance evaluation via shadow traffic, it provides confidence that when the model is introduced in an A/B testing setting, you won’t be fighting system performance issues. Instead, you can focus on measuring user and business outcomes, knowing that latency, stability, and throughput are unlikely to be the bottlenecks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
